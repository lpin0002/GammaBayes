{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><b><I> General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and general setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lpin0002/anaconda3/envs/please/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"gammabayes/BFCalc/\")\n",
    "sys.path.append(\"gammabayes\")\n",
    "\n",
    "from gammabayes.BFCalc.createspectragrids import singlechannel_diffflux, getspectrafunc, darkmatterdoubleinput, energymassinputspectralfunc\n",
    "from gammabayes.utils.utils import log10eaxistrue, longitudeaxistrue, latitudeaxistrue, log10eaxis, longitudeaxis, latitudeaxis, time,psf, edisp, bkgdist, interpolate, special, integrate\n",
    "from gammabayes.utils.utils  import SkyCoord, WcsGeom, inverse_transform_sampling, tqdm#, setup_full_fake_signal_dist_copy_vers#, setup_full_fake_signal_dist, diff_irf_marg\n",
    "from gammabayes.hyperparameter_likelihood import hyperparameter_likelihood\n",
    "from gammabayes.prior import discrete_logprior\n",
    "from gammabayes.likelihood import discrete_loglikelihood\n",
    "from gammabayes.utils.utils  import edisp_test, psf_test, log10eaxis, longitudeaxis, latitudeaxis\n",
    "from gammabayes.utils.utils  import psf_efficient, edisp_efficient, edisp_test, psf_test, single_likelihood, read_config_file\n",
    "from gammabayes.SS_DM_Prior import SS_DM_dist\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy import units as u\n",
    "from scipy import special,stats\n",
    "from scipy.integrate import simps\n",
    "from matplotlib import cm\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "import functools\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# For later in the notebook\n",
    "from gammabayes.utils.utils import confidence_ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "import time, pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "astrophysicalbackground = np.load(\"gammabayes/package_data/unnormalised_astrophysicalbackground.npy\")\n",
    "psfnormalisationvalues = np.load(\"gammabayes/package_data/psfnormalisation.npy\")\n",
    "edispnormalisationvalues = np.load(\"gammabayes/package_data/edispnormalisation.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(751, 35, 30) (18, 15)\n"
     ]
    }
   ],
   "source": [
    "log10emeshtrue, lonmeshtrue, latmeshtrue = np.meshgrid(log10eaxistrue, longitudeaxistrue, latitudeaxistrue, indexing='ij')\n",
    "lonmeshrecon, latmeshrecon = np.meshgrid(longitudeaxis, latitudeaxis, indexing='ij')\n",
    "\n",
    "logjacobtrue = np.meshgrid(np.log(10**log10eaxistrue), longitudeaxistrue, latitudeaxistrue, indexing='ij')[0]\n",
    "\n",
    "\n",
    "print(lonmeshtrue.shape, lonmeshrecon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path: example_run_script.yaml\n",
      "5.64432025\n"
     ]
    }
   ],
   "source": [
    "inputs = read_config_file('example_run_script.yaml')\n",
    "\n",
    "nsig = int(round(inputs['Nevents']*inputs['xi']))\n",
    "nbkg = int(round(inputs['Nevents']*(1-inputs['xi'])))\n",
    "\n",
    "startertimer = time.perf_counter()\n",
    "print(startertimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <h1><b>Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unnormed_logbkgpriorvalues = np.logaddexp(np.squeeze(bkgdist(log10emeshtrue, lonmeshtrue,latmeshtrue)),np.log(astrophysicalbackground))\n",
    "\n",
    "\n",
    "logbkgpriorvalues = unnormed_logbkgpriorvalues - special.logsumexp(unnormed_logbkgpriorvalues+logjacobtrue)\n",
    "\n",
    "logbkgpriorvalues.shape\n",
    "\n",
    "\n",
    "nuisancemesh = np.meshgrid(log10eaxistrue, longitudeaxistrue, latitudeaxistrue, indexing='ij')\n",
    "\n",
    "unnormed_logbkgpriorvalues = np.logaddexp(np.squeeze(bkgdist(*nuisancemesh)),np.log(astrophysicalbackground))\n",
    "# unnormed_logbkgpriorvalues = np.squeeze(bkgdist(*nuisancemesh))\n",
    "\n",
    "\n",
    "logbkgfunc_annoying = interpolate.RegularGridInterpolator((log10eaxistrue, longitudeaxistrue, latitudeaxistrue), np.exp(unnormed_logbkgpriorvalues))\n",
    "logbkgfunc = lambda logenergy, longitude, latitude: np.log(logbkgfunc_annoying((logenergy, longitude, latitude)))\n",
    "\n",
    "\n",
    "bkg_prior = discrete_logprior(logfunction=logbkgfunc, name='Background Prior',\n",
    "                               axes=(log10eaxistrue, longitudeaxistrue, latitudeaxistrue,), \n",
    "                               axes_names=['energy', 'lon', 'lat'], logjacob=logjacobtrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SS_DM_dist_instance= SS_DM_dist(longitudeaxistrue, latitudeaxistrue)\n",
    "logDMpriorfunc = SS_DM_dist_instance.func_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discrete log prior class\n",
       "----------------------------\n",
       "name = Scalar Singlet Dark Matter Prior\n",
       "logfunction type is <function SS_DM_dist.func_setup.<locals>.DM_signal_dist at 0x7faf5848dc10>\n",
       "input units of None\n",
       "over axes ['energy', 'lon', 'lat']\n",
       "with hyperparameter(s) ['mass']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DM_prior = discrete_logprior(logfunction=logDMpriorfunc, name='Scalar Singlet Dark Matter Prior',\n",
    "                               axes=(log10eaxistrue, longitudeaxistrue, latitudeaxistrue,), axes_names=['energy', 'lon', 'lat'],\n",
    "                               default_hyperparameter_values=(inputs[\"logmass\"],), hyperparameter_names=['mass'], logjacob=logjacobtrue)\n",
    "DM_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Value Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if inputs['xi']!=0.0:\n",
    "    siglogevals,siglonvals,siglatvals  = DM_prior.sample(nsig)\n",
    "else:\n",
    "    siglogevals = np.asarray([])\n",
    "    siglonvals = np.asarray([])\n",
    "    siglatvals = np.asarray([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if inputs['xi']!=1.0:\n",
    "    bkglogevals,bkglonvals,bkglatvals  = bkg_prior.sample(nbkg)\n",
    "else:\n",
    "    bkglogevals = np.asarray([])\n",
    "    bkglonvals = np.asarray([])\n",
    "    bkglatvals = np.asarray([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructed Value Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logjacob = np.meshgrid(np.log(10**log10eaxis), longitudeaxis, latitudeaxis, indexing='ij')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input dimensions 1\n",
      "Axes shape: (151,)\n"
     ]
    }
   ],
   "source": [
    "logjacob = np.log(10**log10eaxis)\n",
    "edisp_like = discrete_loglikelihood(logfunction=edisp_test, \n",
    "                                    axes=(log10eaxis,), axes_names='log10E recon',\n",
    "                                    name='energy dispersion',\n",
    "                                    dependent_axes=(log10eaxistrue, longitudeaxistrue, latitudeaxistrue,), logjacob=logjacob,\n",
    "                                    dependent_axes_names = ['log10E true', 'lon', 'lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input dimensions 2\n",
      "Number of data dimensions 2\n",
      "Axes shape: (18, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "discrete log likelihood class\n",
       "---------------------------------\n",
       "name = point spread function \n",
       "logfunction type is <function psf_test at 0x7faf58f8d310>\n",
       "input units of None\n",
       "over axes ['longitude recon', 'latitude recon']\n",
       "with dependent axes ['log10E true', 'lon', 'lat']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psf_like = discrete_loglikelihood(logfunction=psf_test, \n",
    "                                    axes=(longitudeaxis, latitudeaxis), axes_names=['longitude recon', 'latitude recon'],\n",
    "                                    name='point spread function ',\n",
    "                                    dependent_axes=(log10eaxistrue, longitudeaxistrue, latitudeaxistrue,),\n",
    "                                    dependent_axes_names = ['log10E true', 'lon', 'lat'])\n",
    "psf_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 1563.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if inputs['xi']!=0.0:\n",
    "    # sig_log10e_edisp_samples = [edisp_like.sample(signal_event_tuple, 1).tolist() for signal_event_tuple in tqdm(sig_samples.T)]\n",
    "    signal_log10e_measured = [np.squeeze(edisp_like.sample((logeval,*coord,), numsamples=1)) for logeval,coord  in notebook_tqdm(zip(siglogevals, np.array([siglonvals, siglatvals]).T), total=nsig)]\n",
    "else:\n",
    "    signal_log10e_measured = np.asarray([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [00:04<00:00, 172.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "signal_lon_measured = []\n",
    "signal_lat_measured = []\n",
    "\n",
    "if inputs['xi']!=0:\n",
    "    \n",
    "    sig_lonlat_psf_samples =  [psf_like.sample((logeval,*coord,), 1) for logeval,coord  in notebook_tqdm(zip(siglogevals, np.array([siglonvals, siglatvals]).T), total=nsig)]\n",
    "    \n",
    "    for sig_lonlat_psf_sample in sig_lonlat_psf_samples:\n",
    "        signal_lon_measured.append(sig_lonlat_psf_sample[0])\n",
    "        signal_lat_measured.append(sig_lonlat_psf_sample[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1565.72it/s]\n"
     ]
    }
   ],
   "source": [
    "if inputs['xi']!=1.0:\n",
    "    # sig_log10e_edisp_samples = [edisp_like.sample(signal_event_tuple, 1).tolist() for signal_event_tuple in tqdm(sig_samples.T)]\n",
    "    bkg_log10e_measured = [np.squeeze(edisp_like.sample((logeval,*coord,), numsamples=1)) for logeval,coord  in notebook_tqdm(zip(bkglogevals, np.array([bkglonvals, bkglatvals]).T), total=nbkg)]\n",
    "else:\n",
    "    bkg_log10e_measured = np.asarray([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 171.92it/s]\n"
     ]
    }
   ],
   "source": [
    "bkg_lon_measured = []\n",
    "bkg_lat_measured = []\n",
    "\n",
    "if inputs['xi']!=1.0:\n",
    "    \n",
    "    bkg_lonlat_psf_samples =  [psf_like.sample((logeval,*coord,), 1) for logeval,coord  in notebook_tqdm(zip(bkglogevals, np.array([bkglonvals, bkglatvals]).T), total=nbkg)]\n",
    "    \n",
    "    for bkg_lonlat_psf_sample in bkg_lonlat_psf_samples:\n",
    "        bkg_lon_measured.append(bkg_lonlat_psf_sample[0])\n",
    "        bkg_lat_measured.append(bkg_lonlat_psf_sample[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final simulation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    measured_log10e = list(signal_log10e_measured)+list(bkg_log10e_measured)\n",
    "    measured_lon = list(signal_lon_measured)+list(bkg_lon_measured)\n",
    "    measured_lat = list(signal_lat_measured)+list(bkg_lat_measured)\n",
    "    \n",
    "except:\n",
    "    if type(bkg_log10e_measured)==np.float64:\n",
    "        measured_log10e = list(signal_log10e_measured)\n",
    "        measured_lon = list(signal_lon_measured)\n",
    "        measured_lat = list(signal_lat_measured)\n",
    "        measured_log10e.append(bkg_log10e_measured)\n",
    "        measured_lon.append(bkg_lon_measured)\n",
    "        measured_lat.append(bkg_lat_measured)\n",
    "        \n",
    "    elif type(signal_log10e_measured)==np.float64:\n",
    "        measured_log10e = list(bkg_log10e_measured)\n",
    "        measured_lon = list(bkg_lon_measured)\n",
    "        measured_lat = list(bkg_lat_measured)\n",
    "        measured_log10e.append(signal_log10e_measured)\n",
    "        measured_lon.append(signal_lon_measured)\n",
    "        measured_lat.append(signal_lat_measured)\n",
    "    else:\n",
    "        print('what')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><b>Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logmasswindowwidth      = 7/np.sqrt(nsig)\n",
    "\n",
    "logmasslowerbound       = inputs['logmass']-logmasswindowwidth\n",
    "logmassupperbound       = inputs['logmass']+logmasswindowwidth\n",
    "\n",
    "# if 1:\n",
    "if logmasslowerbound<log10eaxis[0]:\n",
    "    logmasslowerbound = log10eaxis[0]\n",
    "# if 1:\n",
    "if logmassupperbound>2:\n",
    "    logmassupperbound = 2\n",
    "\n",
    "\n",
    "logmassrange            = np.linspace(logmasslowerbound, logmassupperbound, inputs['nbins_logmass']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up prior matrices:   0%|                          | 0/2 [00:00<?, ?it/s]\n",
      "Setting up matrices for the 1th prior:   0%|                       | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Setting up matrices for the 1th prior:   2%|▎              | 1/41 [00:01<01:12,  1.81s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:   5%|▋              | 2/41 [00:03<01:10,  1.80s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:   7%|█              | 3/41 [00:05<01:07,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  10%|█▍             | 4/41 [00:07<01:05,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  12%|█▊             | 5/41 [00:08<01:03,  1.77s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  15%|██▏            | 6/41 [00:10<01:01,  1.77s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  17%|██▌            | 7/41 [00:12<00:59,  1.76s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  20%|██▉            | 8/41 [00:14<00:58,  1.76s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  22%|███▎           | 9/41 [00:15<00:56,  1.76s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  24%|███▍          | 10/41 [00:17<00:54,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  27%|███▊          | 11/41 [00:19<00:52,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  29%|████          | 12/41 [00:21<00:50,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  32%|████▍         | 13/41 [00:22<00:49,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  34%|████▊         | 14/41 [00:24<00:47,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  37%|█████         | 15/41 [00:26<00:45,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  39%|█████▍        | 16/41 [00:28<00:43,  1.75s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  41%|█████▊        | 17/41 [00:29<00:42,  1.77s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  44%|██████▏       | 18/41 [00:31<00:40,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  46%|██████▍       | 19/41 [00:33<00:39,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  49%|██████▊       | 20/41 [00:35<00:37,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  51%|███████▏      | 21/41 [00:37<00:35,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  54%|███████▌      | 22/41 [00:38<00:33,  1.78s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  56%|███████▊      | 23/41 [00:41<00:34,  1.91s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  59%|████████▏     | 24/41 [00:42<00:31,  1.87s/it]\u001b[A\n",
      "Setting up matrices for the 1th prior:  61%|████████▌     | 25/41 [00:44<00:29,  1.84s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "hyperparameter_likelihood_instance = hyperparameter_likelihood(priors=(DM_prior, bkg_prior,), likelihood=single_likelihood, \n",
    "                                                               dependent_axes=(log10eaxistrue,  longitudeaxistrue, latitudeaxistrue), \n",
    "                                                               dependent_logjacob=logjacobtrue,\n",
    "                                                               hyperparameter_axes_tuple = ((logmassrange,), (None,)), \n",
    "                                                               numcores=inputs['numcores'], \n",
    "                                                               likelihoodnormalisation = psfnormalisationvalues+edispnormalisationvalues)\n",
    "\n",
    "measured_log10e = [float(measured_log10e_val) for measured_log10e_val in measured_log10e]\n",
    "margresults = hyperparameter_likelihood_instance.full_obs_marginalisation(axisvals= (measured_log10e, measured_lon, measured_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "margresultsarray = np.array(hyperparameter_likelihood_instance.log_margresults)\n",
    "sigmargresults = np.squeeze(np.vstack(margresultsarray[:,0])).T\n",
    "bkgmargresults = np.squeeze(np.vstack(margresultsarray[:,1])).T\n",
    "sigmargresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,3))\n",
    "plt.pcolormesh(*np.meshgrid(list(range(inputs['totalevents'])), logmassrange, indexing='ij'), \n",
    "               sigmargresults.T-special.logsumexp(sigmargresults, axis=1), \n",
    "               cmap='viridis', vmin=-10)\n",
    "plt.ylabel(r'$log_{10}m_\\chi$ [TeV]')\n",
    "plt.axhline(inputs['logmass'], c='tab:orange')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "nbinslambda            = 81\n",
    "lambdawindowwidth      = 9/np.sqrt(inputs['totalevents'])\n",
    "\n",
    "\n",
    "lambdalowerbound       = inputs['xi']-lambdawindowwidth\n",
    "lambdaupperbound       = inputs['xi']+lambdawindowwidth\n",
    "\n",
    "\n",
    "\n",
    "if lambdalowerbound<0:\n",
    "    lambdalowerbound = 0\n",
    "if lambdaupperbound>1:\n",
    "    lambdaupperbound = 1\n",
    "\n",
    "\n",
    "lambdarange            = np.linspace(lambdalowerbound, lambdaupperbound, inputs['nbins_xi']) \n",
    "\n",
    "# log_posterior = []\n",
    "\n",
    "# for lambdaval in notebook_tqdm(lambdarange, total=lambdarange.shape[0]):\n",
    "#     log_posterior.append([np.sum(np.logaddexp(np.log(lambdaval)+sigmargresults[logmassindex,:], np.log(1-lambdaval)+bkgmargresults)) for logmassindex in range(len(list(logmassrange)))])\n",
    "\n",
    "# log_posterior = np.array(log_posterior)-special.logsumexp(log_posterior)\n",
    "\n",
    "hyperparameter_likelihood_instance.create_mixture_log_posterior(mixture_axes = (lambdarange, 1-lambdarange,))\n",
    "\n",
    "log_posterior = np.squeeze(hyperparameter_likelihood_instance.unnormed_log_posterior)\n",
    "endertimer = time.perf_counter()\n",
    "print(endertimer-startertimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><b>Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_posterior = log_posterior-special.logsumexp(log_posterior)\n",
    "\n",
    "colormap = cm.get_cmap('Blues_r', 4)\n",
    "\n",
    "fig, ax = plt.subplots(2,2, dpi=100, figsize=(10,8))\n",
    "plt.suptitle(f\"Nevents= {inputs['totalevents']}\", size=24)\n",
    "\n",
    "# Upper left plot\n",
    "logmass_logposterior = special.logsumexp(log_posterior, axis=0)\n",
    "\n",
    "normalisedlogmassposterior = np.exp(logmass_logposterior-special.logsumexp(logmass_logposterior))\n",
    "\n",
    "cdflogmassposterior = np.cumsum(normalisedlogmassposterior)\n",
    "mean = logmassrange[np.abs(norm.cdf(0)-cdflogmassposterior).argmin()]\n",
    "zscores = [-3, -2,-1,1,2, 3]\n",
    "logmasspercentiles = []\n",
    "for zscore in zscores:\n",
    "    logmasspercentiles.append(logmassrange[np.abs(norm.cdf(zscore)-cdflogmassposterior).argmin()])\n",
    "\n",
    "\n",
    "ax[0,0].plot(logmassrange,normalisedlogmassposterior, c='tab:green')\n",
    "\n",
    "ax[0,0].axvline(mean, c='tab:green', ls=':')\n",
    "\n",
    "\n",
    "for o, percentile in enumerate(logmasspercentiles):\n",
    "            color = colormap(np.abs(zscores[o])/4-0.01)\n",
    "\n",
    "            ax[0,0].axvline(percentile, c=color, ls=':')\n",
    "ax[0,0].axvline(inputs['logmass'], ls='--', color=\"tab:orange\")\n",
    "\n",
    "\n",
    "if min(mean - logmasspercentiles)>log10eaxistrue[1]-log10eaxistrue[0]:\n",
    "    for logetrueval in log10eaxistrue:\n",
    "        ax[0,0].axvline(logetrueval, c='forestgreen', alpha=0.3)\n",
    "ax[0,0].set_ylim([0, None])\n",
    "ax[0,0].set_xlim([logmassrange[0], logmassrange[-1]])\n",
    "\n",
    "# Upper right plot\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "\n",
    "# Lower left plot\n",
    "# ax[1,0].pcolormesh(logmassrange, lambdarange, np.exp(normalisedlogposterior).T, cmap='Blues')\n",
    "ax[1,0].pcolormesh(logmassrange, lambdarange, np.exp(log_posterior), vmin=0)\n",
    "ax[1,0].axvline(inputs['logmass'], c='tab:orange')\n",
    "ax[1,0].axhline(inputs['xi'], c='tab:orange')\n",
    "ax[1,0].set_xlabel(r'$log_{10}$ mass [TeV]')\n",
    "ax[1,0].set_ylabel(r'$\\xi$')\n",
    "\n",
    "ax[1,0].set_ylim([lambdarange[0], lambdarange[-1]])\n",
    "ax[1,0].set_xlim([logmassrange[0], logmassrange[-1]])\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "# I have no clue how this works but I've checked it against some standard distributions and it seems correct\n",
    "try:\n",
    "    normed_posterior = np.exp(log_posterior)/np.exp(log_posterior).sum()\n",
    "    n = 100000\n",
    "    t = np.linspace(0, normed_posterior.max(), n)\n",
    "    integral = ((normed_posterior >= t[:, None, None]) * normed_posterior).sum(axis=(1,2))\n",
    "\n",
    "    from scipy import interpolate\n",
    "    f = interpolate.interp1d(integral, t)\n",
    "    t_contours = f(np.array([1-np.exp(-4.5),1-np.exp(-2.0),1-np.exp(-0.5)]))\n",
    "    ax[1,0].contour(normed_posterior, t_contours, extent=[logmassrange[0],logmassrange[-1], lambdarange[0],lambdarange[-1]], colors='white', linewidths=0.5)\n",
    "except:\n",
    "    warnings.warn(\"Couldn't create contour lines for posterior.\")\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "lambda_logposterior = special.logsumexp(log_posterior, axis=1)\n",
    "\n",
    "normalisedlambdaposterior = np.exp(lambda_logposterior-special.logsumexp(lambda_logposterior))\n",
    "\n",
    "cdflambdaposterior = np.cumsum(normalisedlambdaposterior)\n",
    "meanlambda = lambdarange[np.abs(norm.cdf(0)-cdflambdaposterior).argmin()]\n",
    "lambdapercentiles = []\n",
    "for zscore in zscores:\n",
    "    lambdapercentile = lambdarange[np.abs(norm.cdf(zscore)-cdflambdaposterior).argmin()]\n",
    "    lambdapercentiles.append(lambdapercentile)\n",
    "    print(np.sqrt(1e5/1e8)*np.abs(lambdapercentile - meanlambda))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[1,1].plot(lambdarange,normalisedlambdaposterior, c='tab:green')\n",
    "\n",
    "ax[1,1].axvline(meanlambda, c='tab:green', ls=':')\n",
    "\n",
    "\n",
    "for o, percentile in enumerate(lambdapercentiles):\n",
    "            color = colormap(np.abs(zscores[o])/4-0.01)\n",
    "\n",
    "            ax[1,1].axvline(percentile, c=color, ls=':')\n",
    "ax[1,1].axvline(inputs['xi'], ls='--', color=\"tab:orange\")\n",
    "ax[1,1].set_xlabel(r'$\\xi$')\n",
    "ax[1,1].set_ylim([0, None])\n",
    "\n",
    "\n",
    "# plt.savefig(time.strftime(f\"Figures/TestFigures/{Nsamples}events_lm{truelogmass}_l{truelambda}_%m%d_%H%M.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameter_likelihood_instance.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.system('say Your code is finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testarray1 = np.array([-1, 2, 3, 4 , 5])\n",
    "testarray2 = np.array([0.0, -1.25, -0.25, 0.4, 1.0])\n",
    "testarray3 = np.array([0.0, 0.25, -0.3, -0.4, -0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[print(item) for item in zip(testarray1.tolist(), testarray2.tolist(), testarray3.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "X, Y = np.mgrid[-3:3:100j, -3:3:100j]\n",
    "\n",
    "print(-3:3:100j)\n",
    "z1 = stats.multivariate_normal([0., 0.], [[.5, 0.], [0., .5]])\n",
    "Z1 = z1.pdf(np.dstack((X, Y)))\n",
    "\n",
    "z2 = stats.multivariate_normal([0.5, 0.5], [[.4, 0.], [0., .4]])\n",
    "Z2 = z2.pdf(np.dstack((X, Y)))\n",
    "\n",
    "z3 = stats.multivariate_normal([-1.5, 0.], [[.6, 0.], [0., .2]])\n",
    "Z3 = z3.pdf(np.dstack((X, Y)))\n",
    "\n",
    "z = Z1 + Z2 + Z3\n",
    "z = z / z.sum()\n",
    "\n",
    "n = 1000\n",
    "t = np.linspace(0, z.max(), n)\n",
    "integral = ((z >= t[:, None, None]) * z).sum(axis=(1,2))\n",
    "integral = []\n",
    "for t_val in t:\n",
    "    result = (z >= t_val) \n",
    "    print(result)\n",
    "    integral.append((result*z).sum())\n",
    "\n",
    "from scipy import interpolate\n",
    "f = interpolate.interp1d(integral, t)\n",
    "t_contours = f([1.0 - np.exp(-0.5 * 2.0) ** 2, 1.0 - np.exp(-0.5 * 1.5) ** 2, 1.0 - np.exp(-0.5 * 1.0) ** 2,1.0 - np.exp(-0.5 * 0.5) ** 2])\n",
    "plt.imshow(z.T, origin='lower', extent=[-3,3,-3,3], cmap=\"GnBu\")\n",
    "plt.contour(z.T, t_contours, extent=[-3,3,-3,3], cmap='Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
